{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "321b8871ead54638",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Tecnológico de Monterrey Campus Monterrey\n",
    "# TC3002B Desarrollo de Aplicaciones Avanzadas de Ciencias Computacionales (Grupo 507)\n",
    "\n",
    "## “Convolutional Neural Networks”\n",
    "\n",
    "## Alumno: Romel Aldair Vázquez Molina\n",
    "## Matrícula: A01700519\n",
    "\n",
    "## Profesor: Jorge Mario Cruz Duarte\n",
    "\n",
    "### Fecha de entrega: 31 de mayo del 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45705ce6",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f7d40",
   "metadata": {},
   "source": [
    "This baseline model will be the one you must beat in the next experiments. So, implement a basic CNN model with the following architecture:\n",
    "\n",
    "\n",
    "• Convolutional layer with 32 filters, 3×3 kernel size, and ReLU activation.\n",
    "\n",
    "• MaxPooling layer with 2×2 pool size.\n",
    "\n",
    "• Flatten layer.\n",
    "\n",
    "• Dense layer with 128 units and ReLU activation.\n",
    "\n",
    "• Dense output layer with softmax activation.\n",
    "\n",
    "After compiling the model, train it on the CIFAR-10 dataset and report the accuracy. Consider that this model would be the one to beat in the following experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d835062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "RELU = 'relu'\n",
    "SOFTMAX = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea357b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos del CIFAR 10\n",
    "DatosCIFAR10 = datasets.cifar10.load_data()\n",
    "(EntrenamientoDeImagenes, EntrenamientoDeEtiquetas), (PruebaDeImagenes, PruebaDeEtiquetas ) = DatosCIFAR10\n",
    "\n",
    "# Normalización de los pixeles\n",
    "EntrenamientoDeImagenes = EntrenamientoDeImagenes / 255\n",
    "PruebaDeImagenes =  PruebaDeImagenes / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab62432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento 1 del modelo base\n",
    "# Capa convucional de 32 filtro, con un kernel de 3x3 y activación ReLU\n",
    "# Agrupación Máxima de 2x2\n",
    "# Capa Aplanada\n",
    "# Densidad de la capa con 128 unidades y activación ReLU\n",
    "# Capa de salida con activacion softmax\n",
    "\n",
    "\n",
    "modelo1 = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation= RELU, input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation= RELU),\n",
    "    layers.Dense(10, activation= SOFTMAX)\n",
    "])\n",
    "\n",
    "modelo1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ed32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 17ms/step - accuracy: 0.4178 - loss: 1.6290 - val_accuracy: 0.5803 - val_loss: 1.2036\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.6008 - loss: 1.1462 - val_accuracy: 0.6132 - val_loss: 1.0907\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.6489 - loss: 0.9992 - val_accuracy: 0.6128 - val_loss: 1.0987\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.6858 - loss: 0.9085 - val_accuracy: 0.6463 - val_loss: 1.0089\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.7113 - loss: 0.8250 - val_accuracy: 0.6431 - val_loss: 1.0315\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.7342 - loss: 0.7553 - val_accuracy: 0.6376 - val_loss: 1.0359\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.7501 - loss: 0.7074 - val_accuracy: 0.6399 - val_loss: 1.0697\n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.7740 - loss: 0.6456 - val_accuracy: 0.6569 - val_loss: 1.0561\n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 18ms/step - accuracy: 0.7897 - loss: 0.6042 - val_accuracy: 0.6570 - val_loss: 1.0583\n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 16ms/step - accuracy: 0.8088 - loss: 0.5439 - val_accuracy: 0.6382 - val_loss: 1.1263\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del Modelo\n",
    "# Nota: Sea paciente con el entrenamiento de esta linea de codigo\n",
    "modelo1Entrenado = False\n",
    "\n",
    "if modelo1Entrenado == False:\n",
    "    entrenamiento1 = modelo1.fit(EntrenamientoDeImagenes, EntrenamientoDeEtiquetas, epochs=10, \n",
    "                    validation_data=(PruebaDeImagenes, PruebaDeEtiquetas))\n",
    "    modelo1Entrenado = True\n",
    "\n",
    "else:\n",
    "    print(\"Modelo 1 ya entrenado previamente, cambie la condicional inicial si lo quiere re-entrenar\")    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcee4fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - 4ms/step - accuracy: 0.6382 - loss: 1.1263\n",
      "Precision del Modelo 1: 63.82%\n",
      "Perdida del Modelo 1: 1.13\n"
     ]
    }
   ],
   "source": [
    "# Precision del Modelo\n",
    "resultadosModelo1 = modelo1.evaluate(PruebaDeImagenes, PruebaDeEtiquetas, verbose=2)\n",
    "precisionModelo1 = resultadosModelo1 [1] * 100\n",
    "perdidaModelo1 = resultadosModelo1 [0] \n",
    "\n",
    "print('Precision del Modelo 1: %.2f%%' % precisionModelo1)\n",
    "print(f'Perdida del Modelo 1: {perdidaModelo1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5082424",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ca8ed",
   "metadata": {},
   "source": [
    "Vary the architecture of your baseline model and report how each change impacts the performance. Consider adding\n",
    "more convolutional layers, varying the number of filters in each convolutional layer, including dropout layers to reduce\n",
    "overfitting, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fbefd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with additional convolutional layers\n",
    "modelo2 = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation= RELU, input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(128, (3, 3), activation= RELU), #Buscar lo que hace esto\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation= RELU ), #buscar que hace esto\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation= SOFTMAX)\n",
    "])\n",
    "\n",
    "modelo2.compile(optimizer='adam',\n",
    "                               loss='sparse_categorical_crossentropy',\n",
    "                               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9d0c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 27ms/step - accuracy: 0.3089 - loss: 1.8746 - val_accuracy: 0.5279 - val_loss: 1.3252\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 28ms/step - accuracy: 0.4978 - loss: 1.3923 - val_accuracy: 0.5835 - val_loss: 1.1606\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 19ms/step - accuracy: 0.5563 - loss: 1.2450 - val_accuracy: 0.6385 - val_loss: 1.0219\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 19ms/step - accuracy: 0.5958 - loss: 1.1453 - val_accuracy: 0.6666 - val_loss: 0.9868\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.6166 - loss: 1.0848 - val_accuracy: 0.6596 - val_loss: 0.9519\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 19ms/step - accuracy: 0.6352 - loss: 1.0335 - val_accuracy: 0.6697 - val_loss: 0.9446\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 21ms/step - accuracy: 0.6524 - loss: 0.9797 - val_accuracy: 0.6973 - val_loss: 0.8791\n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - accuracy: 0.6642 - loss: 0.9519 - val_accuracy: 0.6904 - val_loss: 0.9004\n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 23ms/step - accuracy: 0.6795 - loss: 0.9189 - val_accuracy: 0.7038 - val_loss: 0.8526\n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - accuracy: 0.6934 - loss: 0.8749 - val_accuracy: 0.6981 - val_loss: 0.8707\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del Modelo 2\n",
    "# Nota: Sea paciente con el entrenamiento de esta linea de codigo\n",
    "\n",
    "modelo2Entrenado = False\n",
    "\n",
    "if modelo2Entrenado == False:\n",
    "    entrenamiento2 = modelo2.fit(EntrenamientoDeImagenes, EntrenamientoDeEtiquetas, epochs=10, \n",
    "                    validation_data=(PruebaDeImagenes, PruebaDeEtiquetas))\n",
    "    modelo2Entrenado = True\n",
    "\n",
    "else:\n",
    "    print(\"Modelo 2 ya entrenado previamente, cambie la condicional inicial si lo quiere re-entrenar\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20f7e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - 7ms/step - accuracy: 0.6981 - loss: 0.8707\n",
      "Precision del Modelo 2: 69.81%\n",
      "Perdida del Modelo 2: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Precision del Modelo 2\n",
    "#Poner Accuracy, Perdida\n",
    "#Matriz de confusión\n",
    "#Tabla comparativa \n",
    "resultadosModelo2 = modelo2.evaluate(PruebaDeImagenes, PruebaDeEtiquetas, verbose=2)\n",
    "precisionModelo2 = resultadosModelo2 [1] * 100\n",
    "perdidaModelo2 = resultadosModelo2 [0] \n",
    "\n",
    "print('Precision del Modelo 2: %.2f%%' % precisionModelo2)\n",
    "print(f'Perdida del Modelo 2: {perdidaModelo2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3888021",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0639ff",
   "metadata": {},
   "source": [
    "Explore different hyper-parameters, including learning rate, batch size, and number of epochs. Plus, use a validation\n",
    "set to tune the hyper-parameters and report the results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "175dd76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definicion de los hyperparamteros\n",
    "aprendizajes = [0.01, 0.001, 0.0001]\n",
    "lotes = [32, 64, 128]\n",
    "epocas = [5, 10, 15]\n",
    "\n",
    "entrenamientos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c5e3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento de los modelos con distintos hyper paramteros\n",
    "#Se usan la misma arquitectura del modelo 1, ya que esta es la más rapida de entrenar\n",
    "modelo3Entrenado = False\n",
    "\n",
    "if modelo3Entrenado == False:\n",
    "    for lote in lotes:\n",
    "        for epoca in epocas:\n",
    "            for aprendizaje in aprendizajes:\n",
    "                modelo3 = models.Sequential([\n",
    "                    layers.Conv2D(32, (3, 3), activation= RELU, input_shape=(32, 32, 3)),\n",
    "                    layers.MaxPooling2D((2, 2)),\n",
    "                    layers.Flatten(),\n",
    "                    layers.Dense(128, activation= RELU),\n",
    "                    layers.Dense(10, activation= SOFTMAX)\n",
    "                ])\n",
    "                optimizador = tf.keras.optimizers.Adam(learning_rate=aprendizaje) \n",
    "                modelo3.compile(optimizer=optimizador,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "                \n",
    "                entrenamiento3 = modelo3.fit(EntrenamientoDeImagenes, EntrenamientoDeEtiquetas, epochs=epoca, batch_size=lote, validation_split=0.2, verbose=0)\n",
    "                resultadosModelo3 = modelo3.evaluate(PruebaDeImagenes, PruebaDeEtiquetas, verbose=0)\n",
    "                precisionModelo3 = resultadosModelo3 [1]\n",
    "                perdidaModelo3 = resultadosModelo3 [0]\n",
    "                entrenamientos.append((aprendizaje, lote, epoca, precisionModelo3, perdidaModelo3))\n",
    "    modelo3Entrenado = True\n",
    "\n",
    "else:\n",
    "    print(\"Modelo 3 ya entrenado previamente, cambie la condicional inicial si lo quiere re-entrenar\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23e52bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 32  cantidad de epocas -> 5  precision de -> % 10.0  perdida de -> 2.31\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 32  cantidad de epocas -> 5  precision de -> % 60.68  perdida de -> 1.13\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 32  cantidad de epocas -> 5  precision de -> % 55.73  perdida de -> 1.26\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 32  cantidad de epocas -> 10  precision de -> % 10.0  perdida de -> 2.3\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 32  cantidad de epocas -> 10  precision de -> % 60.76  perdida de -> 1.28\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 32  cantidad de epocas -> 10  precision de -> % 60.44  perdida de -> 1.13\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 32  cantidad de epocas -> 15  precision de -> % 35.75  perdida de -> 1.82\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 32  cantidad de epocas -> 15  precision de -> % 62.08  perdida de -> 1.42\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 32  cantidad de epocas -> 15  precision de -> % 62.57  perdida de -> 1.07\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 64  cantidad de epocas -> 5  precision de -> % 36.51  perdida de -> 1.8\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 64  cantidad de epocas -> 5  precision de -> % 59.24  perdida de -> 1.17\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 64  cantidad de epocas -> 5  precision de -> % 54.87  perdida de -> 1.29\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 64  cantidad de epocas -> 10  precision de -> % 38.56  perdida de -> 1.74\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 64  cantidad de epocas -> 10  precision de -> % 62.72  perdida de -> 1.15\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 64  cantidad de epocas -> 10  precision de -> % 58.24  perdida de -> 1.2\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 64  cantidad de epocas -> 15  precision de -> % 46.85  perdida de -> 2.14\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 64  cantidad de epocas -> 15  precision de -> % 64.01  perdida de -> 1.3\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 64  cantidad de epocas -> 15  precision de -> % 61.0  perdida de -> 1.11\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 128  cantidad de epocas -> 5  precision de -> % 49.63  perdida de -> 1.45\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 128  cantidad de epocas -> 5  precision de -> % 60.27  perdida de -> 1.15\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 128  cantidad de epocas -> 5  precision de -> % 50.31  perdida de -> 1.43\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 128  cantidad de epocas -> 10  precision de -> % 54.24  perdida de -> 1.82\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 128  cantidad de epocas -> 10  precision de -> % 64.03  perdida de -> 1.06\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 128  cantidad de epocas -> 10  precision de -> % 57.53  perdida de -> 1.23\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.01  tamanio del lote -> 128  cantidad de epocas -> 15  precision de -> % 39.3  perdida de -> 1.71\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.001  tamanio del lote -> 128  cantidad de epocas -> 15  precision de -> % 62.23  perdida de -> 1.13\n",
      "Experimento con hiperparametros: tasa de aprendizaje -> 0.0001  tamanio del lote -> 128  cantidad de epocas -> 15  precision de -> % 60.01  perdida de -> 1.14\n"
     ]
    }
   ],
   "source": [
    "for entrenamiento in entrenamientos:\n",
    "    print(\"Experimento con hiperparametros: tasa de aprendizaje ->\", entrenamiento[0], \" tamanio del lote ->\", entrenamiento[1], \" cantidad de epocas ->\", entrenamiento[2],\" precision de -> %\", round(entrenamiento[3]*100,2), \" perdida de ->\", round(entrenamiento[4],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9808505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores hiperparametros del modelo 3 tienen una tasa de aprendizaje de:  0.001 , tamanio de lote de:  128 , cantidad de epocas:  10 , precision de: % 64.03 ,perdida de:  1.06\n"
     ]
    }
   ],
   "source": [
    "#Despliegue del mejor modelo con los mejores hyper parametros\n",
    "mejorResultado = max(entrenamientos, key=lambda x: x[3])\n",
    "print('Los mejores hiperparametros del modelo 3 tienen una tasa de aprendizaje de: ', mejorResultado[0], ', tamanio de lote de: ', mejorResultado[1], ', cantidad de epocas: ', mejorResultado[2], ', precision de: %', round(mejorResultado[3] *100,2), ',perdida de: ', round(mejorResultado[4],2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de88bf",
   "metadata": {},
   "source": [
    "## Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35122f5",
   "metadata": {},
   "source": [
    "Consider implementing one or more advanced techniques, such as batch normalisation, data augmentation, or transfer\n",
    "learning. (Even you could try more exotique architectures.) Hence, comparing the model’s performance with these\n",
    "techniques to the baseline model will be easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783d395",
   "metadata": {},
   "source": [
    "Normalización de Batch y Aumento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa7ba3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicación de la normalización de Batch\n",
    "#Se aplica a la arquitectura del Modelo 1 porque es el más sencillo de todos los Modelos\n",
    "modelo4 = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation= RELU, input_shape=(32, 32, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation= RELU),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation= SOFTMAX)\n",
    "])\n",
    "\n",
    "modelo4.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb4b8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición del aumento de datos\n",
    "aumentoDeDatos = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "aumentoDeDatos.fit(EntrenamientoDeImagenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1131e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravml\\OneDrive\\Escritorio\\decimo semestre\\COMPILADORES\\El otro profe\\M2.5\\M2.5\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 117ms/step - accuracy: 0.6738 - loss: 0.9190 - val_accuracy: 0.6204 - val_loss: 1.1990\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 67ms/step - accuracy: 0.6769 - loss: 0.9127 - val_accuracy: 0.5845 - val_loss: 1.4638\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 70ms/step - accuracy: 0.6807 - loss: 0.8978 - val_accuracy: 0.6262 - val_loss: 1.2186\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.6873 - loss: 0.8885 - val_accuracy: 0.6566 - val_loss: 1.0297\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 63ms/step - accuracy: 0.6960 - loss: 0.8727 - val_accuracy: 0.6998 - val_loss: 0.8668\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 67ms/step - accuracy: 0.6963 - loss: 0.8652 - val_accuracy: 0.6616 - val_loss: 0.9958\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.7000 - loss: 0.8568 - val_accuracy: 0.6853 - val_loss: 0.9479\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 63ms/step - accuracy: 0.7019 - loss: 0.8502 - val_accuracy: 0.6650 - val_loss: 1.0527\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 59ms/step - accuracy: 0.7012 - loss: 0.8507 - val_accuracy: 0.6872 - val_loss: 0.9343\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 59ms/step - accuracy: 0.7118 - loss: 0.8247 - val_accuracy: 0.7026 - val_loss: 0.8648\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento del Modelo 4\n",
    "modelo4Entrenado = False\n",
    "\n",
    "if modelo4Entrenado == False:\n",
    "    entrenamiento4 = modelo4.fit(aumentoDeDatos.flow(EntrenamientoDeImagenes, EntrenamientoDeEtiquetas, batch_size=64), epochs=10, \n",
    "                    validation_data=(PruebaDeImagenes, PruebaDeEtiquetas))\n",
    "    modelo4Entrenado = True\n",
    "\n",
    "else:\n",
    "    print(\"Modelo 4 ya entrenado previamente, cambie la condicional inicial si lo quiere re-entrenar\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20b37618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - 3ms/step - accuracy: 0.7026 - loss: 0.8648\n",
      "Precision del Modelo 4: 70.26%\n",
      "Perdida del Modelo 2:  0.86\n"
     ]
    }
   ],
   "source": [
    "# Precision del Modelo 4\n",
    "resultadosModelo4 = modelo4.evaluate(PruebaDeImagenes, PruebaDeEtiquetas, verbose=2)\n",
    "precisionModelo4 = resultadosModelo4 [1] * 100\n",
    "perdidaModelo4 = resultadosModelo4 [0] \n",
    "\n",
    "print('Precision del Modelo 4: %.2f%%' % precisionModelo4)\n",
    "print('Perdida del Modelo 2: ', round(perdidaModelo4,2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
